from ragflow_sdk import RAGFlow
from ragflow_sdk.modules.chat import Chat
import os
import time
import sys

def create_ragflow_resources_multi_docs(page_documents, page_files, pdf_filename, api_key, base_url="http://localhost:8080", custom_dataset_name=None, custom_assistant_name=None):
    """
    ä½¿ç”¨å¤šä¸ªç‹¬ç«‹é¡µé¢æ–‡æ¡£åˆ›å»ºRAGFlowçŸ¥è¯†åº“å’ŒèŠå¤©åŠ©æ‰‹

    å‚æ•°:
    - page_documents: é¡µé¢æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«pageã€contentã€title
    - page_files: é¡µé¢æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    - pdf_filename: PDFæ–‡ä»¶å
    - api_key: RAGFlow APIå¯†é’¥
    - base_url: RAGFlowåŸºç¡€URL
    - custom_dataset_name: è‡ªå®šä¹‰çŸ¥è¯†åº“åç§° (å¯é€‰)
    - custom_assistant_name: è‡ªå®šä¹‰åŠ©æ‰‹åç§° (å¯é€‰)
    """
    print(f"åˆ›å»ºRAGFlowèµ„æºï¼Œä½¿ç”¨{len(page_documents)}ä¸ªç‹¬ç«‹é¡µé¢æ–‡æ¡£")

    try:
        # åˆå§‹åŒ–RAGFlowå®¢æˆ·ç«¯
        rag_object = RAGFlow(api_key=api_key, base_url=base_url)

        # åˆ›å»ºæ•°æ®é›†åç§°
        if custom_dataset_name:
            dataset_name = custom_dataset_name
        else:
            base_name = os.path.splitext(os.path.basename(pdf_filename))[0]
            dataset_name = f"{base_name}_çŸ¥è¯†åº“"

        print(f"åˆ›å»ºå¤šæ–‡æ¡£æ•°æ®é›†: {dataset_name}")

        # åˆ é™¤å·²å­˜åœ¨çš„çŸ¥è¯†åº“å’ŒåŠ©æ‰‹
        try:
            # æŸ¥æ‰¾å¹¶åˆ é™¤å·²å­˜åœ¨çš„çŸ¥è¯†åº“
            existing_datasets = rag_object.list_datasets()
            dataset_ids_to_delete = []
            for ds in existing_datasets:
                if ds.name == dataset_name:
                    print(f"å‘ç°å·²å­˜åœ¨çš„çŸ¥è¯†åº“ '{dataset_name}'ï¼Œæ­£åœ¨åˆ é™¤...")
                    dataset_ids_to_delete.append(ds.id)

            if dataset_ids_to_delete:
                rag_object.delete_datasets(dataset_ids_to_delete)
                print("å·²åˆ é™¤æ—§çš„çŸ¥è¯†åº“")

            # æŸ¥æ‰¾å¹¶åˆ é™¤å·²å­˜åœ¨çš„èŠå¤©åŠ©æ‰‹
            if custom_assistant_name:
                assistant_name = custom_assistant_name
            else:
                base_name = os.path.splitext(os.path.basename(pdf_filename))[0]
                assistant_name = f"{base_name}_åŠ©æ‰‹"
            existing_chats = rag_object.list_chats()
            chat_ids_to_delete = []
            for chat in existing_chats:
                if chat.name == assistant_name:
                    print(f"å‘ç°å·²å­˜åœ¨çš„èŠå¤©åŠ©æ‰‹ '{assistant_name}'ï¼Œæ­£åœ¨åˆ é™¤...")
                    chat_ids_to_delete.append(chat.id)

            if chat_ids_to_delete:
                rag_object.delete_chats(chat_ids_to_delete)
                print("å·²åˆ é™¤æ—§çš„èŠå¤©åŠ©æ‰‹")

        except Exception as cleanup_e:
            print(f"æ¸…ç†å·²å­˜åœ¨èµ„æºæ—¶å‡ºé”™: {str(cleanup_e)}")
            print("ç»§ç»­åˆ›å»ºæ–°çš„èµ„æº...")

        # åˆ›å»ºæ•°æ®é›†ï¼ˆç®€åŒ–é…ç½®ï¼Œè®©æ¯ä¸ªæ–‡æ¡£å¤©ç„¶æˆä¸ºä¸€ä¸ªåˆ†å—ï¼‰
        dataset = rag_object.create_dataset(
            name=dataset_name,
            description="å¤šé¡µé¢ç»´ä¿®æ¡ˆä¾‹æ–‡æ¡£ï¼Œæ¯é¡µç‹¬ç«‹åˆ†å—",
            embedding_model="BAAI/bge-large-zh-v1.5@BAAI",
            chunk_method="naive"
        )
        print(f"æ•°æ®é›† '{dataset_name}' åˆ›å»ºæˆåŠŸ")

        print("é…ç½®å¤šæ–‡æ¡£åˆ†å—ç­–ç•¥ï¼šæ¯ä¸ªæ–‡æ¡£ç‹¬ç«‹åˆ†å—")
        dataset.update({
            "parser_config": {
                "chunk_token_num": 1000,  # å¤§åˆ†å—ç¡®ä¿æ¯é¡µå®Œæ•´
                "html4excel": False,
                "raptor": {"use_raptor": False}
            }
        })
        print("æ•°æ®é›†é…ç½®æ›´æ–°æˆåŠŸ")
        print("  - ç­–ç•¥: æ¯ä¸ªé¡µé¢æ–‡æ¡£å¤©ç„¶ç‹¬ç«‹åˆ†å—")
        print("  - åˆ†å—å¤§å°: 1000 tokens (ç¡®ä¿å•é¡µå®Œæ•´)")

        # å‡†å¤‡å¤šä¸ªé¡µé¢æ–‡æ¡£è¿›è¡Œæ‰¹é‡ä¸Šä¼ 
        docs_to_upload = []
        for page_doc, page_file in zip(page_documents, page_files):
            print(f"å‡†å¤‡ä¸Šä¼ ç¬¬{page_doc['page']}é¡µæ–‡æ¡£: {page_file}")

            encoded_text = page_doc['content'].encode('utf-8')
            docs_to_upload.append({
                "display_name": page_file,
                "blob": encoded_text
            })

        # æ‰¹é‡ä¸Šä¼ æ‰€æœ‰é¡µé¢æ–‡æ¡£
        print(f"æ‰¹é‡ä¸Šä¼ {len(docs_to_upload)}ä¸ªé¡µé¢æ–‡æ¡£...")
        dataset.upload_documents(docs_to_upload)
        print("æ‰€æœ‰é¡µé¢æ–‡æ¡£ä¸Šä¼ æˆåŠŸ")

        # ç­‰å¾…æ–‡æ¡£è§£æå®Œæˆ
        print("å¼€å§‹è§£ææ–‡æ¡£...")
        docs = dataset.list_documents()
        doc_ids = [doc.id for doc in docs if hasattr(doc, 'id')]
        print(f"å¼€å§‹è§£ææ–‡æ¡£ï¼ŒID: {doc_ids}")

        if doc_ids:
            dataset.async_parse_documents(doc_ids)
            print("æ–‡æ¡£ä¸Šä¼ æˆåŠŸï¼Œæ­£åœ¨è§£æ...")

            # ç­‰å¾…æ–‡æ¡£è§£æå®Œæˆ
            all_done = False
            max_wait_time = 300  # æœ€é•¿ç­‰å¾…5åˆ†é’Ÿ
            start_time = time.time()

            while not all_done and (time.time() - start_time) < max_wait_time:
                all_done = True
                for doc_id in doc_ids:
                    docs_check = dataset.list_documents(id=doc_id)
                    if docs_check and len(docs_check) > 0:
                        doc_status = docs_check[0].run
                        print(f"æ–‡æ¡£ {doc_id} çŠ¶æ€: {doc_status}")
                        if doc_status != "DONE":
                            all_done = False
                    else:
                        all_done = False

                if not all_done:
                    print("æ–‡æ¡£ä»åœ¨è§£æä¸­ï¼Œç­‰å¾…10ç§’...")
                    time.sleep(10)

            print("æ–‡æ¡£è§£æå®Œæˆï¼")

            # æ£€æŸ¥æœ€ç»ˆåˆ†å—æ•°
            total_chunks = 0
            for doc_id in doc_ids:
                try:
                    chunks = dataset.get_chunks(document_id=doc_id)
                    chunk_count = len(chunks) if chunks else 0
                    total_chunks += chunk_count
                    print(f"æ–‡æ¡£ {doc_id} è§£æå‡º {chunk_count} ä¸ªåˆ†å—")
                except:
                    print(f"æ— æ³•è·å–æ–‡æ¡£ {doc_id} çš„åˆ†å—ä¿¡æ¯")
            print(f"æ€»å…±è§£æå‡º {total_chunks} ä¸ªåˆ†å—")

        # åˆ›å»ºèŠå¤©åŠ©æ‰‹
        if custom_assistant_name:
            assistant_name = custom_assistant_name
        else:
            base_name = os.path.splitext(os.path.basename(pdf_filename))[0]
            assistant_name = f"{base_name}_åŠ©æ‰‹"
        print(f"åˆ›å»ºèŠå¤©åŠ©æ‰‹: {assistant_name}")

        assistant = rag_object.create_chat(
            name=assistant_name,
            dataset_ids=[dataset.id]
        )


        prompt_template = """
# æŒ–æ˜æœºç»´ä¿®ä¸“å®¶

ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„æŒ–æ˜æœºç»´ä¿®æŠ€æœ¯ä¸“å®¶ï¼Œä¸“é—¨è§£ç­”æŒ–æ˜æœºç»´ä¿®é—®é¢˜ã€‚è¯·ä»”ç»†é˜…è¯»ä¸‹æ–¹çš„çŸ¥è¯†åº“å†…å®¹ï¼Œå¹¶ç»“åˆä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

## æ ¸å¿ƒè¦æ±‚

### ğŸ“‹ å›ç­”ç»“æ„
1. **é—®é¢˜è¯†åˆ«**ï¼šç¡®è®¤æ•…éšœç±»å‹å’Œè®¾å¤‡å‹å·
2. **æ¡ˆä¾‹å¼•ç”¨**ï¼šå…³è”ç›¸å…³ç»´ä¿®æ¡ˆä¾‹
3. **è¯Šæ–­æ­¥éª¤**ï¼šæŒ‰ç®€å•åˆ°å¤æ‚é¡ºåºè¯´æ˜æ’æŸ¥æ–¹æ³•
4. **ç»´ä¿®æ–¹æ¡ˆ**ï¼šæä¾›å…·ä½“æ“ä½œæ­¥éª¤å’Œæ‰€éœ€é›¶ä»¶
5. **é¢„é˜²å»ºè®®**ï¼šè¯´æ˜å¦‚ä½•é¿å…ç±»ä¼¼æ•…éšœ

### ğŸ–¼ï¸ å›¾ç‰‡é“¾æ¥å¤„ç†ï¼ˆæå…¶é‡è¦ï¼‰
- **ä¸¥æ ¼è¦æ±‚**ï¼šåœ¨å›ç­”ä¸­å¿…é¡»å®Œæ•´ã€åŸå°ä¸åŠ¨åœ°è¾“å‡ºæ¡ˆä¾‹ä¸­çš„å›¾ç‰‡é“¾æ¥
- **æ ¼å¼ä¿æŒ**ï¼š<img src="http://localhost:9000/ragflow-demo/xxx.png" alt="ç»´ä¿®å›¾ç‰‡" width="300">
- **ç¦æ­¢ä¿®æ”¹**ï¼šä¸å¾—æ›´æ”¹URLã€æ–‡ä»¶åæˆ–ä»»ä½•å‚æ•°
- **å¿…è¦å¼•ç”¨**ï¼šæ¶‰åŠç»´ä¿®æ­¥éª¤æ—¶ï¼Œå¿…é¡»å¼•ç”¨å¯¹åº”å›¾ç‰‡

### âš ï¸ é‡è¦æé†’
- åªåŸºäºçŸ¥è¯†åº“æ¡ˆä¾‹å›ç­”ï¼Œä¸åšä¸»è§‚æ¨æµ‹
- å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥ã€‚
- å¼ºè°ƒå®‰å…¨ç¬¬ä¸€ï¼Œæé†’ä¸“ä¸šäººå‘˜æ“ä½œ

Here is the knowledge base:
{knowledge}
The above is the knowledge base.
        """

        # æ›´æ–°åŠ©æ‰‹é…ç½®ï¼Œå‚è€ƒragflow_build.pyçš„é…ç½®
        assistant.update({
            "prompt": {
                "prompt": prompt_template.strip(),
                "show_quote": True,  # ä¿æŒæ˜¾ç¤ºå¼•ç”¨
                "top_n": 8  # æ£€ç´¢æ›´å¤šç›¸å…³å†…å®¹
            }
        })

        print(f"èŠå¤©åŠ©æ‰‹ '{assistant_name}' åˆ›å»ºå¹¶é…ç½®å®Œæˆ")

        return dataset, assistant

    except Exception as e:
        print(f"åˆ›å»ºRAGFlowå¤šæ–‡æ¡£èµ„æºæ—¶å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        raise